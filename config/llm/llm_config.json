{
  "default_provider": "anthropic",
  "default_model": "claude-sonnet-4.5",
  "fallback_provider": "mock",
  "refresh_interval_seconds": 600,
  "providers": {
    "anthropic": {
      "enabled": true,
      "api_key_env": "ANTHROPIC_API_KEY",
      "base_url": "https://api.anthropic.com",
      "models": {
        "claude-opus-4": {
          "version": "20250514",
          "model_id": "claude-opus-4-20250514",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 15.00,
          "cost_per_1m_output_tokens": 75.00,
          "description": "Most powerful model. Best for complex analysis, deep reasoning, advanced coding, research synthesis, and tasks requiring highest intelligence.",
          "best_for": [
            "complex reasoning",
            "advanced coding",
            "research synthesis",
            "multi-step problem solving",
            "creative writing"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "claude-sonnet-4.5": {
          "version": "20250929",
          "model_id": "claude-sonnet-4-5-20250929",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 3.00,
          "cost_per_1m_output_tokens": 15.00,
          "description": "Smartest model, efficient for everyday use. Excellent balance of intelligence, speed, and cost. Best for general workflows, data analysis, and automation.",
          "best_for": [
            "workflow planning",
            "data analysis",
            "code generation",
            "task automation",
            "general intelligence tasks"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "claude-sonnet-4": {
          "version": "20250514",
          "model_id": "claude-sonnet-4-20250514",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 3.00,
          "cost_per_1m_output_tokens": 15.00,
          "description": "Fast and efficient. Great for everyday tasks, content generation, and quick analysis.",
          "best_for": [
            "content generation",
            "quick analysis",
            "everyday tasks",
            "customer support"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "claude-haiku-4": {
          "version": "20250514",
          "model_id": "claude-haiku-4-20250514",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 0.80,
          "cost_per_1m_output_tokens": 4.00,
          "description": "Fastest and most cost-effective. Ideal for simple tasks, data extraction, and high-volume processing.",
          "best_for": [
            "data extraction",
            "simple classification",
            "high-volume processing",
            "quick responses"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        }
      }
    },
    "openai": {
      "enabled": true,
      "api_key_env": "OPENAI_API_KEY",
      "base_url": "https://api.openai.com/v1",
      "models": {
        "gpt-4o": {
          "version": "2024-11-20",
          "model_id": "gpt-4o-2024-11-20",
          "enabled": true,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 2.50,
          "cost_per_1m_output_tokens": 10.00,
          "description": "Most advanced OpenAI model. Excellent for multimodal tasks, vision, and complex reasoning.",
          "best_for": [
            "multimodal tasks",
            "vision analysis",
            "complex reasoning",
            "code generation"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "gpt-4-turbo": {
          "version": "2024-04-09",
          "model_id": "gpt-4-turbo-2024-04-09",
          "enabled": true,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 10.00,
          "cost_per_1m_output_tokens": 30.00,
          "description": "High-performance GPT-4 with vision capabilities. Great for complex tasks requiring deep understanding.",
          "best_for": [
            "complex analysis",
            "vision tasks",
            "detailed responses"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "gpt-4o-mini": {
          "version": "2024-07-18",
          "model_id": "gpt-4o-mini-2024-07-18",
          "enabled": true,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 0.15,
          "cost_per_1m_output_tokens": 0.60,
          "description": "Fast and affordable. Perfect for simple tasks, data processing, and high-volume operations.",
          "best_for": [
            "simple tasks",
            "data processing",
            "high-volume operations",
            "cost-sensitive applications"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "o1-preview": {
          "version": "2024-09-12",
          "model_id": "o1-preview-2024-09-12",
          "enabled": true,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 15.00,
          "cost_per_1m_output_tokens": 60.00,
          "description": "Reasoning model optimized for complex problem-solving, mathematics, and scientific analysis.",
          "best_for": [
            "complex reasoning",
            "mathematics",
            "scientific analysis",
            "step-by-step problem solving"
          ],
          "supports_vision": false,
          "supports_function_calling": false,
          "supports_streaming": false
        }
      }
    },
    "google": {
      "enabled": true,
      "api_key_env": "GOOGLE_API_KEY",
      "base_url": "https://generativelanguage.googleapis.com/v1beta",
      "models": {
        "gemini-2.0-flash-exp": {
          "version": "latest",
          "model_id": "gemini-2.0-flash-exp",
          "enabled": true,
          "max_tokens": 1000000,
          "context_window": 1000000,
          "cost_per_1m_input_tokens": 0.00,
          "cost_per_1m_output_tokens": 0.00,
          "description": "Latest experimental Gemini. Ultra-fast with massive context window. Great for document analysis and processing.",
          "best_for": [
            "document analysis",
            "massive context processing",
            "multimodal tasks",
            "experimental features"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "gemini-1.5-pro": {
          "version": "002",
          "model_id": "gemini-1.5-pro-002",
          "enabled": true,
          "max_tokens": 2000000,
          "context_window": 2000000,
          "cost_per_1m_input_tokens": 1.25,
          "cost_per_1m_output_tokens": 5.00,
          "description": "Most capable Gemini. Excellent for long-context tasks, multimodal analysis, and complex reasoning.",
          "best_for": [
            "long-context analysis",
            "multimodal tasks",
            "complex reasoning",
            "document understanding"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "gemini-1.5-flash": {
          "version": "002",
          "model_id": "gemini-1.5-flash-002",
          "enabled": true,
          "max_tokens": 1000000,
          "context_window": 1000000,
          "cost_per_1m_input_tokens": 0.075,
          "cost_per_1m_output_tokens": 0.30,
          "description": "Fast and efficient. Perfect for high-volume operations and quick responses with large contexts.",
          "best_for": [
            "high-volume operations",
            "quick responses",
            "cost-effective processing",
            "large context handling"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        }
      }
    },
    "meta": {
      "enabled": true,
      "api_key_env": "META_API_KEY",
      "base_url": "https://api.llama-api.com",
      "models": {
        "llama-3.3-70b": {
          "version": "latest",
          "model_id": "llama-3.3-70b-instruct",
          "enabled": true,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 0.60,
          "cost_per_1m_output_tokens": 0.60,
          "description": "Latest Llama model. Excellent for general tasks, multilingual support, and cost-effective processing.",
          "best_for": [
            "general tasks",
            "multilingual support",
            "cost-effective solutions",
            "open-source deployments"
          ],
          "supports_vision": false,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "llama-3.1-405b": {
          "version": "latest",
          "model_id": "llama-3.1-405b-instruct",
          "enabled": false,
          "max_tokens": 128000,
          "context_window": 128000,
          "cost_per_1m_input_tokens": 2.70,
          "cost_per_1m_output_tokens": 2.70,
          "description": "Largest Llama model. Best for complex reasoning and advanced tasks requiring maximum capability.",
          "best_for": [
            "complex reasoning",
            "advanced tasks",
            "research applications"
          ],
          "supports_vision": false,
          "supports_function_calling": true,
          "supports_streaming": true
        }
      }
    },
    "deepseek": {
      "enabled": true,
      "api_key_env": "DEEPSEEK_API_KEY",
      "base_url": "https://api.deepseek.com/v1",
      "models": {
        "deepseek-chat": {
          "version": "latest",
          "model_id": "deepseek-chat",
          "enabled": true,
          "max_tokens": 64000,
          "context_window": 64000,
          "cost_per_1m_input_tokens": 0.14,
          "cost_per_1m_output_tokens": 0.28,
          "description": "Cost-effective Chinese AI model. Excellent for coding, mathematics, and multilingual tasks.",
          "best_for": [
            "coding tasks",
            "mathematics",
            "Chinese language",
            "cost-effective solutions"
          ],
          "supports_vision": false,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "deepseek-coder": {
          "version": "latest",
          "model_id": "deepseek-coder",
          "enabled": true,
          "max_tokens": 64000,
          "context_window": 64000,
          "cost_per_1m_input_tokens": 0.14,
          "cost_per_1m_output_tokens": 0.28,
          "description": "Specialized for code generation and analysis. Excellent for software development tasks.",
          "best_for": [
            "code generation",
            "code review",
            "debugging",
            "software development"
          ],
          "supports_vision": false,
          "supports_function_calling": true,
          "supports_streaming": true
        }
      }
    },
    "aws_bedrock": {
      "enabled": true,
      "api_key_env": "AWS_ACCESS_KEY_ID",
      "base_url": null,
      "region": "us-east-1",
      "models": {
        "claude-3-sonnet-bedrock": {
          "version": "20240229",
          "model_id": "anthropic.claude-3-sonnet-20240229-v1:0",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 3.00,
          "cost_per_1m_output_tokens": 15.00,
          "description": "Claude 3 Sonnet via AWS Bedrock. Great for enterprise deployments with AWS infrastructure.",
          "best_for": [
            "enterprise deployments",
            "AWS integration",
            "secure environments",
            "regulated industries"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "claude-3-haiku-bedrock": {
          "version": "20240307",
          "model_id": "anthropic.claude-3-haiku-20240307-v1:0",
          "enabled": true,
          "max_tokens": 200000,
          "context_window": 200000,
          "cost_per_1m_input_tokens": 0.25,
          "cost_per_1m_output_tokens": 1.25,
          "description": "Claude 3 Haiku via AWS Bedrock. Fast and cost-effective for high-volume AWS workloads.",
          "best_for": [
            "high-volume processing",
            "AWS integration",
            "cost optimization",
            "quick responses"
          ],
          "supports_vision": true,
          "supports_function_calling": true,
          "supports_streaming": true
        },
        "titan-express-bedrock": {
          "version": "v1",
          "model_id": "amazon.titan-text-express-v1",
          "enabled": true,
          "max_tokens": 8000,
          "context_window": 8000,
          "cost_per_1m_input_tokens": 0.20,
          "cost_per_1m_output_tokens": 0.60,
          "description": "Amazon Titan Express. Fast and cost-effective for simple tasks within AWS.",
          "best_for": [
            "simple tasks",
            "AWS native integration",
            "cost optimization"
          ],
          "supports_vision": false,
          "supports_function_calling": false,
          "supports_streaming": true
        }
      }
    },
    "huggingface": {
      "enabled": true,
      "api_key_env": "HUGGINGFACE_API_KEY",
      "base_url": "https://api-inference.huggingface.co/models",
      "models": {
        "mixtral-8x7b": {
          "version": "latest",
          "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
          "enabled": true,
          "max_tokens": 32000,
          "context_window": 32000,
          "cost_per_1m_input_tokens": 0.00,
          "cost_per_1m_output_tokens": 0.00,
          "description": "Open-source mixture of experts. Great for diverse tasks and experimentation.",
          "best_for": [
            "experimentation",
            "diverse tasks",
            "open-source deployments",
            "research"
          ],
          "supports_vision": false,
          "supports_function_calling": false,
          "supports_streaming": true
        },
        "zephyr-7b": {
          "version": "beta",
          "model_id": "HuggingFaceH4/zephyr-7b-beta",
          "enabled": true,
          "max_tokens": 8000,
          "context_window": 8000,
          "cost_per_1m_input_tokens": 0.00,
          "cost_per_1m_output_tokens": 0.00,
          "description": "Small but capable model. Perfect for lightweight deployments and testing.",
          "best_for": [
            "lightweight deployments",
            "testing",
            "edge computing",
            "resource-constrained environments"
          ],
          "supports_vision": false,
          "supports_function_calling": false,
          "supports_streaming": true
        }
      }
    },
    "mock": {
      "enabled": true,
      "api_key_env": null,
      "base_url": null,
      "models": {
        "mock-llm": {
          "version": "1.0",
          "model_id": "mock-llm-v1",
          "enabled": true,
          "max_tokens": 100000,
          "context_window": 100000,
          "cost_per_1m_input_tokens": 0.00,
          "cost_per_1m_output_tokens": 0.00,
          "description": "Mock LLM for testing and development. Returns predefined responses for common patterns.",
          "best_for": [
            "testing",
            "development",
            "debugging",
            "offline mode"
          ],
          "supports_vision": false,
          "supports_function_calling": false,
          "supports_streaming": false
        }
      }
    }
  }
}
